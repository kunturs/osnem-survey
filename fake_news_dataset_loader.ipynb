{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0b6886a",
   "metadata": {},
   "source": [
    "# Fake News Dataset Loader ðŸ“Š\n",
    "\n",
    "This notebook provides loading examples and structural overviews for various fake news and synthetic text datasets:\n",
    "\n",
    "- LIAR\n",
    "- ISOT\n",
    "- COVID-19 Claim News\n",
    "- GPT-2 Output Dataset\n",
    "- PHEME (Twitter Rumor Threads)\n",
    "\n",
    "Each section includes:\n",
    "- A brief explanation of the dataset's structure\n",
    "- Python code to load and explore the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d15b709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“˜ LIAR Dataset\n",
    "import pandas as pd\n",
    "\n",
    "columns = ['id', 'label', 'statement', 'subject', 'speaker',\n",
    "           'speaker_job', 'state', 'party',\n",
    "           'barely_true', 'false', 'half_true',\n",
    "           'mostly_true', 'pants_on_fire', 'context']\n",
    "\n",
    "liar_train = pd.read_csv('train.tsv', sep='\\t', names=columns)\n",
    "liar_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce76fd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“˜ ISOT Fake News Dataset\n",
    "fake_df = pd.read_csv('Fake.csv')\n",
    "real_df = pd.read_csv('True.csv')\n",
    "\n",
    "fake_df['label'] = 0\n",
    "real_df['label'] = 1\n",
    "\n",
    "isot_df = pd.concat([fake_df, real_df], ignore_index=True)\n",
    "isot_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fc3f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“˜ COVID-19 Claim Dataset\n",
    "covid_df = pd.read_csv('covid19_claims.csv')\n",
    "covid_df[['claim', 'label']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680ed8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“˜ GPT-2 Output Dataset\n",
    "gpt2_df = pd.read_csv('/kaggle/input/xl-1542M-k40.test.csv')\n",
    "webtext_df = pd.read_csv('/kaggle/input/webtext.test.csv')\n",
    "\n",
    "gpt2_df['label'] = 1\n",
    "webtext_df['label'] = 0\n",
    "\n",
    "gpt2_combined = pd.concat([gpt2_df, webtext_df], ignore_index=True)\n",
    "gpt2_combined.sample(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a89c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“˜ PHEME Dataset (Simplified Loader)\n",
    "import os, json\n",
    "\n",
    "folds = ['charliehebdo-all-rnr-threads', 'ottawashooting-all-rnr-threads']\n",
    "texts, usernames = [], []\n",
    "\n",
    "for fold in folds:\n",
    "    for root, _, files in os.walk(f'pheme_root/{fold}'):\n",
    "        for fname in files:\n",
    "            if fname.endswith('.json'):\n",
    "                with open(os.path.join(root, fname)) as f:\n",
    "                    tweet = json.load(f)\n",
    "                    texts.append(tweet.get('text', ''))\n",
    "                    usernames.append(tweet.get('user', {}).get('screen_name', ''))\n",
    "\n",
    "print(f\"Loaded {len(texts)} tweets.\")\n",
    "print(\"Sample tweet:\", texts[0])\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}